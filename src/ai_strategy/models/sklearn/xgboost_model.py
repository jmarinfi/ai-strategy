"""XGBoost model wrapper for trading predictions (Parity with LightGBMModel)."""

import warnings
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import joblib
import numpy as np
import optuna
import pandas as pd
import xgboost as xgb
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import TimeSeriesSplit

# Suppress XGBoost warnings
warnings.filterwarnings("ignore", category=UserWarning)

def add_technical_indicators(df: pd.DataFrame) -> pd.DataFrame:
    """Add technical indicators (RSI, MACD, BB, ATR, Volume, Momentum) to the DataFrame.
    
    Identical implementation to the provided lightgbm_model.py to ensure feature consistency.

    Args:
        df: DataFrame with OHLCV data.

    Returns:
        DataFrame with technical indicators added.
    """
    print("\nðŸ“Š Calculating technical indicators (XGBoost)...")

    # Ensure sorted by timestamp
    df = df.sort_values("timestamp").reset_index(drop=True)

    # Convert numbers to numeric just in case
    cols = ["open", "high", "low", "close", "volume"]
    for col in cols:
        df[col] = pd.to_numeric(df[col], errors="coerce")

    # ===== RSI =====
    for period in [14, 21]:
        delta = df["close"].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss.replace(0, 1e-10)
        df[f"rsi_{period}"] = 100 - (100 / (1 + rs))

    # ===== MACD =====
    exp1 = df["close"].ewm(span=12, adjust=False).mean()
    exp2 = df["close"].ewm(span=26, adjust=False).mean()
    df["macd"] = exp1 - exp2
    df["signal"] = df["macd"].ewm(span=9, adjust=False).mean()

    # ===== Bollinger Bands =====
    window = 20
    df["sma_20"] = df["close"].rolling(window=window).mean()
    df["std_20"] = df["close"].rolling(window=window).std()
    df["upper_band"] = df["sma_20"] + (df["std_20"] * 2)
    df["lower_band"] = df["sma_20"] - (df["std_20"] * 2)

    # ===== ATR (Average True Range) =====
    high_low = df["high"] - df["low"]
    high_close = np.abs(df["high"] - df["close"].shift())
    low_close = np.abs(df["low"] - df["close"].shift())
    ranges = pd.concat([high_low, high_close, low_close], axis=1)
    true_range = np.max(ranges, axis=1)
    df["atr"] = pd.DataFrame(true_range).rolling(window=14).mean()

    # ===== Moving Averages =====
    for window in [7, 14, 50, 200]:
        df[f"sma_{window}"] = df["close"].rolling(window=window).mean()
        df[f"ema_{window}"] = df["close"].ewm(span=window, adjust=False).mean()

    # ===== Volume & Momentum =====
    df["volume_change"] = df["volume"].pct_change()
    df["momentum"] = df["close"] - df["close"].shift(4)

    # Clean NaN values generated by indicators
    df = df.dropna().reset_index(drop=True)
    return df


class XGBoostModel:
    """Wrapper for XGBoost model optimized for trading data.
    
    Replicates the interface of LightGBMModel exactly.
    """

    def __init__(self, model_path: Optional[str] = None):
        """Initialize the model.

        Args:
            model_path: Path to save/load the model. If None, uses default.
        """
        self.model_path = Path(model_path) if model_path else Path("models/xgboost_model.joblib")
        self.model = None
        
        # Define features exactly as in the reference model
        self.feature_cols = [
            "rsi_14", "rsi_21", "macd", "signal",
            "upper_band", "lower_band", "atr",
            "sma_7", "sma_50", "ema_7", "ema_50",
            "volume_change", "momentum"
        ]
        
        # Create directory if it doesn't exist
        self.model_path.parent.mkdir(parents=True, exist_ok=True)

    def prepare_features(self, df: pd.DataFrame) -> np.ndarray:
        """Prepare features for training or prediction.

        Args:
            df: DataFrame with raw data.

        Returns:
            Numpy array with features ready for the model.
        """
        # 1. Add indicators if not present
        if "rsi_14" not in df.columns:
            df = add_technical_indicators(df)

        # 2. Select columns
        missing_cols = [col for col in self.feature_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"Missing columns: {missing_cols}")

        X = df[self.feature_cols].values.astype(np.float32)
        return X

    def train(self, df: pd.DataFrame, target_col: str = "target", n_trials: int = 20) -> Dict[str, float]:
        """Train the model with hyperparameter optimization using Optuna.

        Args:
            df: DataFrame with training data.
            target_col: Name of the target column.
            n_trials: Number of Optuna trials.

        Returns:
            Dictionary with evaluation metrics (accuracy, f1, precision, recall).
        """
        print(f"ðŸš€ Starting XGBoost training with {len(df)} records...")

        # Prepare X and y
        X = self.prepare_features(df)
        y = df[target_col].values.astype(int)

        # Calculate scale_pos_weight for class imbalance
        # In trading, signals (class 1) are often fewer than noise/holds (class 0)
        n_neg = len(y) - np.sum(y)
        n_pos = np.sum(y)
        scale_pos_weight = n_neg / n_pos if n_pos > 0 else 1.0
        
        print(f"âš–ï¸ Class balance: {n_neg} (0) vs {n_pos} (1). scale_pos_weight={scale_pos_weight:.2f}")

        # Time Series Split for validation
        tscv = TimeSeriesSplit(n_splits=5)

        def objective(trial):
            # Hyperparameter search space adapted for XGBoost
            param = {
                "objective": "binary:logistic",
                "eval_metric": "logloss",
                "tree_method": "hist",  # Faster and more robust for continuous features
                "scale_pos_weight": scale_pos_weight, # Handle imbalance
                "grow_policy": trial.suggest_categorical("grow_policy", ["depthwise", "lossguide"]), # lossguide = LightGBM style
                
                "n_estimators": trial.suggest_int("n_estimators", 100, 1000),
                "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
                "max_depth": trial.suggest_int("max_depth", 3, 12),
                "min_child_weight": trial.suggest_int("min_child_weight", 1, 10),
                "subsample": trial.suggest_float("subsample", 0.6, 1.0),
                "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
                "gamma": trial.suggest_float("gamma", 0, 5),
                "reg_alpha": trial.suggest_float("reg_alpha", 1e-5, 10, log=True), # L1 Reg
                "reg_lambda": trial.suggest_float("reg_lambda", 1e-5, 10, log=True), # L2 Reg
                "n_jobs": -1,
                "verbosity": 0,
                "seed": 42
            }

            scores = []
            for train_index, val_index in tscv.split(X):
                X_train, X_val = X[train_index], X[val_index]
                y_train, y_val = y[train_index], y[val_index]

                # Initialize and train
                model = xgb.XGBClassifier(**param)
                
                # Using early stopping explicitly in fit
                model.fit(
                    X_train, 
                    y_train, 
                    eval_set=[(X_val, y_val)], 
                    verbose=False
                )
                
                preds = model.predict(X_val)
                score = f1_score(y_val, preds, average="binary")
                scores.append(score)

            return np.mean(scores)

        # Optimize
        study = optuna.create_study(direction="maximize")
        study.optimize(objective, n_trials=n_trials)

        print(f"âœ… Best parameters found: {study.best_params}")

        # Train final model with best params on all data
        best_params = study.best_params
        # Ensure objective and basic params are included if Optuna didn't select them
        best_params.update({
            "objective": "binary:logistic",
            "tree_method": "hist",
            "scale_pos_weight": scale_pos_weight,
            "n_jobs": -1,
            "verbosity": 0,
            "seed": 42
        })
        
        self.model = xgb.XGBClassifier(**best_params)
        self.model.fit(X, y)

        # Save model
        joblib.dump({"model": self.model, "features": self.feature_cols}, self.model_path)
        print(f"ðŸ’¾ Model saved to {self.model_path}")

        # Final evaluation on the last split for reporting
        train_idx, test_idx = list(tscv.split(X))[-1]
        X_test, y_test = X[test_idx], y[test_idx]
        y_pred = self.model.predict(X_test)

        metrics = {
            "accuracy": float(accuracy_score(y_test, y_pred)),
            "precision": float(precision_score(y_test, y_pred, zero_division=0)),
            "recall": float(recall_score(y_test, y_pred, zero_division=0)),
            "f1": float(f1_score(y_test, y_pred, zero_division=0)),
        }
        
        print(f"ðŸ“ˆ Final metrics: {metrics}")
        return metrics

    def load(self) -> None:
        """Load the trained model from disk."""
        if not self.model_path.exists():
            raise FileNotFoundError(f"Model not found at {self.model_path}")

        data = joblib.load(self.model_path)
        self.model = data["model"]
        self.feature_cols = data["features"]
        print(f"âœ… XGBoost model loaded from {self.model_path}")

    def predict_proba(self, X: np.ndarray) -> Dict[str, float]:
        """Predict probabilities for a single input (or batch, returning last).

        Args:
            X: Features array.

        Returns:
            Dictionary with 'prob_down' and 'prob_up'.
        """
        if self.model is None:
            raise ValueError("Model not loaded. Call load() first.")

        # Get probabilities [prob_class_0, prob_class_1]
        # class_0 = DOWN
        # class_1 = UP
        # XGBoost predict_proba output follows sklearn convention: shape (n_samples, n_classes)
        y_proba = self.model.predict_proba(X)
        
        # Take the last prediction if a batch is sent (matching typical trading use case for 'current candle')
        last_proba = y_proba[-1] if y_proba.ndim > 1 else y_proba

        return {
            "prob_down": float(last_proba[0]),
            "prob_up": float(last_proba[1]),
        }

    def predict(self, features: pd.Series) -> dict[str, float]:
        """End-to-end prediction from raw features.

        Args:
            features: Series with raw feature values.

        Returns:
            Dictionary with 'prob_down' and 'prob_up' probabilities.
        """
        # Convert Series to DataFrame and transpose to match format expected by prepare_features
        # If 'features' is a single row (Series), we make it a DataFrame
        if isinstance(features, pd.Series):
             df = features.to_frame().T
        else:
             df = features

        X = self.prepare_features(df)
        return self.predict_proba(X)

    @property
    def is_loaded(self) -> bool:
        """Check if model is loaded."""
        return self.model is not None

    @property
    def num_features(self) -> int:
        """Get the number of features expected by the model."""
        return len(self.feature_cols)

    def __repr__(self) -> str:
        """String representation of the model."""
        status = "loaded" if self.is_loaded else "not loaded"
        return (
            f"XGBoostModel(path={self.model_path}, "
            f"status={status}, "
            f"features={len(self.feature_cols)})"
        )